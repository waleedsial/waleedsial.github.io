---
title: "Machine Learning: Linear Regression From scratch in Python Using Gradient Descent "
date: 2020-03-14
tags: [machine learning, data science, neural network, Linear Regression ]
header:
  images: "images/linear/Linear_Regression.png"
excerpt: "Machine Learning, Perceptron, Data"

---


### Developing linear & Logistic regression without using any library
The purpose of this post is to get up & running with linear & logistic regression
implementation from scratch, the conceptual understanding, formulas has been taken from
Prof. Andrew Ng Machine Learning course. Following are salient features of this blog:



- Development of linear & logistic regression code from scratch.
- Optimization Algorithm : Gradient Descent
- Programming Language : Python
- Implementation of gradient Descent : Vectorized
- Test Function for gradient descent to check calculated gradients




### Pre-requisite

- Regression Understanding (If you don't understand linear regression, please watch Andrew Ng lectures on Youtube)
- Understanding of formulas which are used in linear regression
- Python
- A prepared dataset to apply the algorithm.


### We will not focus on

- In depth conceptual understanding of regression


If you just need the notebook please go to this link [Github](https://github.com/waleedsial/Linear-Logistic-Regression/blob/master/AML%20V%201.3.ipynb)

# Linear Regression : What, why & How ?

What: Linear regression is a statistical method that we use to model a relationship between variables. We shall take a simple example of housing prices where we have a general idea that if a house is bigger(surface area) than its price will be higher than other houses given all other factors remain constant.
![ Price & Size Relation ](/images/linear/HS.png)

Why : It would be beneficial for us if we can model this relationship between size & price of a house. The benefit of doing this will be manifold in every such scenario. This is the motivation of studying such methods. Hence it explains the why & what part.
 In short, linear regression is statistical modelling technique which can be used to model, predict & quantify relationship between different variables.

How : Let us take a layman approach to understand this & discuss the gruesome details in another post.

How would a layman will approach this problem of housing  ?
Lets say if I am an investor & I would like to buy houses that I think can give me some profit. Let us say that I have only one feature for now as
size of the house. I also assume that I don't have any idea about relationship between size & price of a house.
I will see the size of the houses & their past prices. Using this, I can make a guess about the price of a house & make a decision to buy it.
Initially, it is possible that actual price is different & I incur some loss.
With time, I will try to change my guess so that it becomes closer to actual selling price & gradually I become good at it.
This is what at a very high level any Optimization looks like & gradient descent is an optimization method which we use to model linear relationships.


A word of caution, humans make complex decisions fairly easy, machines don't. They have to learn !


### Functions to develop


- Wrapper/Caller
- Gradient Descent
- Prediction  
- Sigmoid  
- Gradient Calculator
- Gradient Updater
- Convergence Check
- Cost Calculation
- Gradient Checker Function

# Lets us move to the code & explanation of the functions

![ Higher Level Function View  ](/images/linear/Functional_Flow.jpg)


### Wrapper/Caller
Explanation: The purpose of this function was to create a higher level abstraction
for experimenting with parameters. This function pass parameters to other functions.
Sort of a make shift main function.

This function will take parameters to set the following:
- Flag for doing linear or logistic regression
- Alpha value for learning rate
- Number of iterations of training  
- Data Matrix, it is our X values for training
- Y matrix, for training
- Theta Matrix, this contains our weights.
- x_test_matrix, for testing
- y_test_matrix , for testing
- convergence_threshold, this is a criteria for stopping training

Return Values are :
- Optimum Thetas/weights
- Cost History, it contains the cost for each training run.
- Differences in gradients, this needs better explanation
- History of test cost

```python
def wrapper_function(activation,alpha,maximum_iterations,data_matrix,y_matrix,theta_matrix,x_test_matrix,y_test_matrix,convergence_threshold):

        optimum_thetas,Cost_history,gradient_diff_sum_array,test_cost_history= gradient_descent(activation,alpha,data_matrix,theta_matrix,y_matrix,maximum_iterations,x_test_matrix,y_test_matrix,convergence_threshold)
        return optimum_thetas,Cost_history,gradient_diff_sum_array,test_cost_history
```

![ Higher Level Function View  ](/images/linear/Gradient_Descent.jpg)


```python

# One general purpose gradient descent, both regression and classification tasks use same gradient descent function.
# Algorithms are differentiated based on activation.

def gradient_descent(activation,alpha,data_matrix,theta_matrix,y_matrix,maximum_iterations,x_test_matrix,y_test_matrix,convergence_threshold):

    cost_history_array = np.zeros(maximum_iterations) # for each iteration, keep a cost for plotting
    test_cost_history = np.empty(maximum_iterations) # for saving test error

    optimum_thetas = theta_matrix
    updated_thetas = theta_matrix
    gradient_diff_sum_array = np.zeros(maximum_iterations)
    convergence_iteration_number = 0
    #convergence = False
    for i in range(maximum_iterations):

        print('Epoch : ',i)
        old_cost = cost_function(activation,data_matrix,updated_thetas,y_matrix)
        print('Cost :',old_cost)
        cost_history_array[i] = old_cost
        # Following array will save the test error for each epoch updated thetas.
        test_cost_history[i] = cost_function(activation,x_test_matrix,updated_thetas,y_test_matrix)

        gradient = gradient_calculator(activation,data_matrix,updated_thetas,y_matrix,alpha)
        # Checking, for debuggin only
        #gradient_difference = check_gradient(activation,gradient,updated_thetas,data_matrix,y_matrix)
        #gradient_diff_sum_array[i] = np.sum(gradient_difference)

        old_thetas = updated_thetas
        updated_thetas = gradient_updater(gradient,updated_thetas,alpha)

        # Calling convergence check function
        convergence = convergence_check(activation,updated_thetas,old_thetas,data_matrix,y_matrix,convergence_threshold)
        if(convergence):
            #print('convergence has occured, killing the loop')
            optimum_thetas = old_thetas
            convergence_iteration_number = i
            break


        if(i == maximum_iterations-1):
            #print('Try more Epochs for saturation, Did not converge')
            optimum_thetas = updated_thetas

        # Calling gradient checker function
        #gradient_approximation,grad_difference = gradient_checker(gradient,updated_thetas,data_matrix,y_matrix)


        #print('-------------------------------------------------------------------------------------------------------')
    return optimum_thetas,cost_history_array,gradient_diff_sum_array,test_cost_history

```


### Prediction Function
Explanation : This function will return the predicted values given a theta matrix & a data matrix.
It takes the dot product between x values & thetas to give us predicted y values.
```python
def predicted_value(data_matrix,thetas):
    return np.dot(data_matrix,thetas)
```

### Sigmoid Function
Explanation :This function is to be used for logistic regression. It is a simple implementation
of sigmoid function.

```python
def sigmoid_function(z):
    return 1 / (1 + exp(-z))
```


### Sigmoid Caller Function
Explanation : This function calls sigmoid function on the predicted values.

```python
def H_function(data_matrix,thetas):
    H = sigmoid_function(predicted_value(data_matrix,thetas))
    return H
```


### calculate gradient to be subtracted from weights
```python

def gradient_calculator(activation,data_matrix,thetas,y_matrix,alpha):

    Xtheta = predicted_value(data_matrix,thetas)
    data_matrix_T = np.transpose(data_matrix)
    m = data_matrix.shape[0]

    if(activation == 0):
        gradient = (np.dot(data_matrix_T,(Xtheta-y_matrix)))
        return gradient/m

    elif(activation == 1):
        h = H_function(data_matrix,thetas)
        gradient = np.dot(data_matrix_T,(h- y_matrix))
        return gradient/m

```


### Gradient Updater
It takes weights, and subtracts the result of gradient calculator function.
```python
def gradient_updater(gradient,updated_thetas,alpha):
    return updated_thetas-(gradient*alpha)

```


### convergence check function
Checks if convergence has been achieved or not, we stop training epochs if true.
```python
def convergence_check(activation,updated_thetas,old_thetas,data_matrix,y_matrix,convergence_threshold):
    if(activation==0):
        old_cost = cost_function(activation,data_matrix,old_thetas,y_matrix)
        updated_cost = cost_function(activation,data_matrix,updated_thetas,y_matrix)
        if(old_cost-updated_cost< convergence_threshold):
            print('Linear Regression Cost has converged,returning thetas in the 2nd last iteration')
            return True
    if(activation==1):
        old_cost = cost_function(activation,data_matrix,old_thetas,y_matrix)
        updated_cost = cost_function(activation,data_matrix,updated_thetas,y_matrix)

        if(old_cost-updated_cost< convergence_threshold):
            print('logistic regression Cost has converged,returning thetas in the 2nd last iteration')
            return True

```


This function use definition of gradient calculation to test the gradients calculated
This function gets called in the for loop and should be only used for testing purposes.
Not in production. Therefore, the call to this function is commented out at the moment.

For understanding this method of gradient checking, go to the following link:

[Debugging: Gradient Checking Stanford Tutotrial ](http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/)

```python

def check_gradient(activation,gradients,thetas,data_matrix,y_values):
    # initializing empty array
    print('check_gradient called')
    Estimated_gradient = np.zeros((thetas.shape[0], 1))
    gradient_difference = np.zeros((thetas.shape[0], 1))


    for i in range(thetas.shape[0]):
        epsilon_constant = 0.0001
        epsilon_vector = np.zeros((thetas.shape[0], 1))
        epsilon_vector[i,0] = 1
        theta_iPlus = thetas + np.dot(epsilon_vector,epsilon_constant)
        theta_iMinus = thetas - np.dot(epsilon_vector,epsilon_constant)

        Estimated_gradient = cost_function(activation,data_matrix,theta_iPlus,y_values)- cost_function(activation,data_matrix,theta_iMinus,y_values)
        Estimated_gradient = Estimated_gradient/(2*epsilon_constant)

        # so our Estimated_gradient is basically a gradient estimate for the ith theta
        #print('Estimated_gradient for  iteration ',i,Estimated_gradient)
        #print('Actual gradient' ,gradients[i])

        # So idea is to have the difference for the ith theta estimate and actual value
        gradient_difference[i] = Estimated_gradient - gradients[i,0]

    return gradient_difference


```
