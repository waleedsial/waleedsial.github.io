---
title: "Machine Learning: Linear Regression From scratch in Python Using Gradient Descent "
date: 2020-03-14
tags: [machine learning, data science, neural network, Linear Regression ]
header:
  images: "images/linear/Linear_Regression.png"
excerpt: "Machine Learning, Perceptron, Data"

---


### Developing linear & Logistic regression without using any library
The purpose of this post is to get up & running with linear regression
implementation from scratch, it has the following salient features:



- Development of regression code from scratch.
- Optimization Algorithm : Gradient Descent
- Programming Language : Python
- Implementation of gradient Descent : Vectorized
- Test Function for gradient descent to check calculated gradients


### Functions to develop


- Wrapper/Caller
- Gradient Descent
- Prediction  
- Sigmoid  
- Gradient Calculator
- Gradient Updater
- Convergence Check
- Cost Calculation
- Gradient Checker Function

### Pre-requisite

- Linear Regression Understanding
- Understanding of formulas which are used in linear regression
- Python

### We will not focus on

- In depth conceptual understanding of linear regression
- In depth understanding of any features  


If you just need the notebook please go to this link [An absolute link](https://github.com/waleedsial/Linear-Logistic-Regression/blob/master/AML%20V%201.3.ipynb)

# Linear Regression : What, why & How ?

Linear regression is a statistical method which we employ to model a relationship between variables. We shall take a simple example of housing prices where we have a general idea that if a house is bigger(surface area) than its price will be higher than other houses given all other factors remain constant.
![ Price & Size Relation ](/images/linear/HS.png)

It would be beneficial for us if we can model this relationship between size & price of a house. The benefit of doing this will be manifold in every such scenario. This is the motivation of studying such methods. Hence it explains the why & what part.
 In short, linear regression is statistical modelling technique which can be used to model, predict & quantify relationship between different variables.

How is this done ?
Let us take a layman approach to understand this & discuss the gruesome details in another post.

How would a layman will approach this problem of housing  ?
Lets say if I am an investor & I would like to buy houses that I think can give me some profit. Let us say that I have only one feature for now as
size of the house. I also assume that I don't have any idea about relationship between size & price of a house.
I will see the size of the houses & their past prices. Using this, I can make a guess about the price of a house & make a decision to buy it.
Initially, it is possible that actual price is different & I incur some loss.
With time, I will try to change my guess so that it becomes closer to actual selling price & gradually I become good at it.
This is what at a very high level any Optimization looks like & gradient descent is an optimization method which we use to model linear relationships.


A word of caution, humans make complex decisions fairly easy, machines don't. They have to learn !


# Lets us move to the functions that we will need in this development.

### Prediction Function
This function will return the predicted values given a theta matrix & a data matrix.
```python
def predicted_value(data_matrix,thetas):
    return np.dot(data_matrix,thetas)
```

### Sigmoid Function
This function is to be used for logistic regression

```python
def sigmoid_function(z):
    return 1 / (1 + exp(-z))
```


### Sigmoid Caller Function

```python
def H_function(data_matrix,thetas):
    H = sigmoid_function(predicted_value(data_matrix,thetas))
    return H
```


### calculate gradient to be subtracted from weights
```python

def gradient_calculator(activation,data_matrix,thetas,y_matrix,alpha):

    Xtheta = predicted_value(data_matrix,thetas)
    data_matrix_T = np.transpose(data_matrix)
    m = data_matrix.shape[0]

    if(activation == 0):
        gradient = (np.dot(data_matrix_T,(Xtheta-y_matrix)))
        return gradient/m

    elif(activation == 1):
        h = H_function(data_matrix,thetas)
        gradient = np.dot(data_matrix_T,(h- y_matrix))
        return gradient/m

```


### Gradient Updater
It takes weights, and subtracts the result of gradient calculator function.
```python
def gradient_updater(gradient,updated_thetas,alpha):
    return updated_thetas-(gradient*alpha)

```


### convergence check function
Checks if convergence has been achieved or not, we stop training epochs if true.
```python
def convergence_check(activation,updated_thetas,old_thetas,data_matrix,y_matrix,convergence_threshold):
    if(activation==0):
        old_cost = cost_function(activation,data_matrix,old_thetas,y_matrix)
        updated_cost = cost_function(activation,data_matrix,updated_thetas,y_matrix)
        if(old_cost-updated_cost< convergence_threshold):
            print('Linear Regression Cost has converged,returning thetas in the 2nd last iteration')
            return True
    if(activation==1):
        old_cost = cost_function(activation,data_matrix,old_thetas,y_matrix)
        updated_cost = cost_function(activation,data_matrix,updated_thetas,y_matrix)

        if(old_cost-updated_cost< convergence_threshold):
            print('logistic regression Cost has converged,returning thetas in the 2nd last iteration')
            return True

```


This function uses a gradient testing strategy and checks the calculated gradients.
```python

def check_gradient(activation,gradients,thetas,data_matrix,y_values):
    # initializing empty array
    print('check_gradient called')
    Estimated_gradient = np.zeros((thetas.shape[0], 1))
    gradient_difference = np.zeros((thetas.shape[0], 1))


    for i in range(thetas.shape[0]):
        epsilon_constant = 0.0001
        epsilon_vector = np.zeros((thetas.shape[0], 1))
        epsilon_vector[i,0] = 1
        theta_iPlus = thetas + np.dot(epsilon_vector,epsilon_constant)
        theta_iMinus = thetas - np.dot(epsilon_vector,epsilon_constant)

        Estimated_gradient = cost_function(activation,data_matrix,theta_iPlus,y_values)- cost_function(activation,data_matrix,theta_iMinus,y_values)
        Estimated_gradient = Estimated_gradient/(2*epsilon_constant)

        # so our Estimated_gradient is basically a gradient estimate for the ith theta
        #print('Estimated_gradient for  iteration ',i,Estimated_gradient)
        #print('Actual gradient' ,gradients[i])

        # So idea is to have the difference for the ith theta estimate and actual value
        gradient_difference[i] = Estimated_gradient - gradients[i,0]

    return gradient_difference


```
